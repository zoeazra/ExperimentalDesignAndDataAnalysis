---
title: "Assignment 1 - Report"
author: "Eleni Liarou, Zoë Azra Blei, Frederieke Loth, group 20"
date: "23 February 2025"
output:
  pdf_document:
    latex_engine: xelatex
fontsize: 11pt
highlight: tango
---

::: {style="text-align: justify;"}
# Exercise 1: Cholesterol

```{r, echo=FALSE}
data = read.delim("cholesterol.txt", sep=' ')
```

#### Section a

In order to investigate the normality of the data set, Q-Q plots are created below for both the *Before* and *After8weeks* columns. As in both plots the data points closely follow the diagonal red line, the data is approximating a normal distribution. While some minor deviations may be present in the tails, the overall pattern suggests that the normality assumption is reasonable.

```{r, echo=FALSE, fig.width=6, fig.height=3}
par(mfrow = c(1,2)) 

qqnorm(data$Before, main = "Q-Q Plot for Before")
qqline(data$Before, col = "red")

qqnorm(data$After8weeks, main = "Q-Q Plot for After 8 Weeks")
qqline(data$After8weeks, col = "red")

```

```{r, echo=FALSE, fig.width=6, fig.height=3}
par(mfrow = c(1,2))  

hist(data$Before, probability = TRUE, 
     main = 'Distribution for Cholesterol \n level Before margarine',
     xlab = 'Cholesterol level (mmol/L)',
     ylab = 'Probability')
lines(density(data$Before), col = "red", lwd = 2)

hist(data$After8weeks, probability = TRUE, 
     main = 'Distribution for Cholesterol \n level After margarine',
     xlab = 'Cholesterol level (mmol/L)',
     ylab = 'Probability')
lines(density(data$After8weeks), col = "red", lwd = 2)
```

To further explore the normality assumption, above histograms are plotted for both *Before* and *After8weeks*. The histograms exhibit a roughly bell-shaped distribution, which supports the assumption of normality.

However, to address normality more formally, a Shapiro-Wilk test is conducted, as this test is suitable to test for normality for small data sets. The null hypothesis is as follows:

H0: The data is normally distributed.

The W-statistic measures how closely the data aligns with a normal distribution, ranging from 0 to 1, where values closer to 1 indicate a stronger likelihood of normality. Considering the results for *Before* and *After8weeks*, both W-values are close to 1. Additionally, with a 95% confidence level, both p-values exceed 0.05, meaning that we fail to reject H0. These findings provide strong evidence that the data in both columns can be considered to be normally distributed.

```{r}
shapiro_before = shapiro.test(data$Before)
shapiro_after = shapiro.test(data$After8weeks)
```

```{r, echo=FALSE,results='asis'}
cat("**Shapiro-Wilk Test for Before: **",
    "W-statistic =", round(shapiro_before$statistic, 3), ", p-value =", round(shapiro_before$p.value, 3), "\n\n")
cat("**Shapiro-Wilk Test for After 8 Weeks: **",
    "W-statistic =", round(shapiro_after$statistic, 3), ", p-value =", round(shapiro_after$p.value, 3), "\n")
```

```{r, echo=FALSE, fig.width=5, fig.height=3}
plot(data$Before, data$After8weeks, 
     main = 'Regression for Before and After8weeks', 
     xlab='Before', 
     ylab='After')
abline(lm(After8weeks ~ Before, data = data), col = "red")
```

In order to investigate the relationship between the columns of *Before* and *After8weeks* a scatter plot is created above. The scatter plot demonstrates a strong positive correlation between 'Before' and 'After8weeks' cholesterol levels. The data points align closely with the red regression line, suggesting that individuals with higher cholesterol levels before the diet intervention also tend to have higher cholesterol levels after 8 weeks. This indicates that while cholesterol levels may have decreased, there remains a strong relationship between pre- and post-diet measurements.

To quantify this correlation, the Pearson’s correlation coefficient is calculated below. A high Pearson correlation (close to 1) indicates a strong positive relationship between the two columns. The correlation coefficient exhibits a value of approximately 0.99, confirming the strong positive relationship. Additionally, the p-value is smaller than 0.05, indicating that the correlation is statistically significant.

```{r}
cor_result = cor.test(data$Before, data$After8weeks, method = "pearson")
```

```{r, echo=FALSE, results='asis'}
rounded_cor = round(cor_result$estimate, 3)  
rounded_p = formatC(cor_result$p.value, format = "e", digits = 3)  
rounded_conf_int = round(cor_result$conf.int, 3)  

# Print formatted results
cat("**Pearson Correlation Test: **", "\n\n")
cat("Correlation coefficient (r) = ", rounded_cor, ", p-value =", rounded_p, ", 95% CI: [", rounded_conf_int[1], ",", rounded_conf_int[2], "]")
```

#### Section b

Since the cholesterol levels were measured on the same individuals at different times, the data is paired, making a paired t-test appropriate. However, since the t-test assumes normality of the mean difference, we first assess this assumption by applying a Shapiro-Wilk test. H0: The data is normally distributed.

The W-statistic (0.99) is close to 1, and the p-value exceeds 0.05, meaning we fail to reject H0, providing strong evidence that the mean difference follows a normal distribution.

```{r}
difference = data$Before - data$After8weeks
shapiro_result = shapiro.test(difference)
```

```{r, echo=FALSE,results='asis'}
rounded_W = round(shapiro_result$statistic, 3) 
rounded_p = round(shapiro_result$p.value, 3)

# Print formatted results
cat("**Shapiro-Wilk Test for Mean Difference:** W-statistic =", rounded_W, ",", "p-value =", rounded_p)
```

The paired t-test shows strong evidence against the null hypothesis (t = 14.946, p = 1.639e-11), indicating a significant cholesterol reduction after 8 weeks on the margarine diet. The mean difference of 0.629 mmol/L supports this, with a 95% confidence interval [0.556, Inf] confirming that the reduction is at least 0.556 mmol/L. These results suggest a clear effect of the diet on lowering cholesterol levels.

```{r}
t_test_result = t.test(data$Before, data$After8weeks, paired = TRUE, alternative = "greater")
```

```{r, echo=FALSE,results='asis'}
rounded_t = round(t_test_result$statistic, 3)
rounded_p = formatC(t_test_result$p.value, format = "e", digits = 3) 
rounded_conf_int = round(t_test_result$conf.int, 3)

# Print formatted results
cat("**Paired T-Test Results:** \n\n T-statistic =", rounded_t, ", p-value =", rounded_p, ", 95% Confidence Interval: [", rounded_conf_int[1], ",", rounded_conf_int[2], "]")

```

The permutation test (p = 1.0e-05) provides strong evidence to reject the null hypothesis, confirming a significant cholesterol reduction of 0.629 mmol/L after 8 weeks. This supports the effectiveness of the margarine diet. A Mann-Whitney U-test is not applicable as it requires independent samples, while this data is paired.

```{r}
diff = data$Before - data$After8weeks
n_permutations = 100000
observed_mean = mean(diff)

permute_test = function(diff) {
  permuted_diff <- diff * sample(c(-1, 1), length(diff), replace = TRUE)
  return(mean(permuted_diff)) 
}

set.seed(42)
permute_distr = replicate(n_permutations, permute_test(diff))
rounded_observed_mean = round(observed_mean, 3) 
rounded_p_value = formatC(mean(abs(permute_distr) >= abs(observed_mean)), format = "e", digits = 3)
```

```{r, echo=FALSE, results='asis'}
cat("**Observed mean difference:**", rounded_observed_mean, "\n")
cat("**Permutation test p-value:**", rounded_p_value)
```

#### Section c

To estimate the mean cholesterol level after 8 weeks, we compute a 97% confidence interval using both the t-test and bootstrapping. The t-test CI, assuming normality, is [5.164, 6.394], while the bootstrap CI, based on resampling, is [5.230, 6.320]. The substantial overlap suggests that both methods provide similar estimates. The slight differences arise because the t-test relies on the sample standard deviation, whereas bootstrapping does not assume normality. Since both approaches yield consistent results, we can confidently conclude that cholesterol levels remain moderate after 8 weeks on the diet.

```{r}
n = length(data$After8weeks)
sample_mean = mean(data$After8weeks)
sample_sd = sd(data$After8weeks)
critical_value = qt(1-0.015, df=17)
standard_error = sample_sd / sqrt(n)
left_bound = sample_mean - critical_value * standard_error
right_bound = sample_mean + critical_value * standard_error

bootstrap_ci = function(x, conf_level = 0.97, B = 10000) {
  alpha = 1 - conf_level
  Bstats = lapply(1:B, FUN = function(i) {
    boot_sample = sample(x, size = length(x), replace = TRUE)
    mean(boot_sample)
  } )
  Bstats = unlist(Bstats)
  ci = round(quantile(Bstats, prob = c(alpha/2, 1-alpha/2)), 3)
  return(ci)
}
```

```{r, echo=FALSE, results='asis'}
# Round values to 3 decimals
rounded_mean = round(sample_mean, 3)
rounded_sd = round(sample_sd, 3)
rounded_critical_value = round(critical_value, 3)
rounded_left_bound = round(left_bound, 3)
rounded_right_bound = round(right_bound, 3)

# Print formatted results
cat("**Results:**\n\n")
cat("Sample mean:", rounded_mean)
cat(", Sample standard deviation:", rounded_sd)
cat(", Critical value:", rounded_critical_value)
cat(", 97% CI for mu: [", rounded_left_bound, ",", rounded_right_bound, "]\n\n")
set.seed(42)
bootstrap_ci_result <- bootstrap_ci(data$After8weeks)
cat("Bootstrap test with 97% CI: [", bootstrap_ci_result[1], ",", bootstrap_ci_result[2], "]\n")

```

#### Section d

For the bootstrap test, we consider the following null hypothesis:

H0: The data follows a uniform distribution with a minimum of 3 and a maximum of theta.

```{r}
set.seed(42)  # Ensure reproducibility
T_max_observed = max(data$After8weeks)
n = length(data$After8weeks)
theta_values = 3:12
p_values = numeric(length(theta_values))
Bootstraps = 100000 
for (i in seq_along(theta_values)) {
  theta = theta_values[i]
  t_star = numeric(Bootstraps)
  for (j in 1:Bootstraps) {
    x_star = runif(n, min = 3, max = theta)
    t_star[j] = max(x_star)
  }
  pl = sum(t_star < T_max_observed) / Bootstraps  
  pr = sum(t_star > T_max_observed) / Bootstraps  
  p_values[i] = 2 * min(pl, pr)
}
```

```{r, echo=FALSE, results='asis'}
results_table = data.frame(
  Theta = theta_values,
  P_Value = round(p_values, 3)
)
# Transpose the table properly
library(tibble)
horizontal_table <- as.data.frame(t(results_table[,-1]))  # Exclude Theta column for transposing
colnames(horizontal_table) = results_table$Theta  # Use Theta values as column names
rownames(horizontal_table) = "P-Value"  # Set row name
library(knitr)
cat("**Results: Table 1** Theta values and their p-values")
kable(horizontal_table)

```

To generate bootstrap samples from a Uniform(3, theta) distribution, we iterate over theta values from 3 to 12 in steps of 1, generating 100,000 bootstrap samples for each. The maximum value from each re-sample is recorded, and the p-value is computed by comparing the observed maximum(*T_max_observed*) with the bootstrap distribution (*t_star*).

```{r}
accepted_theta <- c(8, 9, 10, 11)
ks_results <- list()

for (theta in accepted_theta) {
  ks_test_result <- ks.test(data$After8weeks, "punif", min = 3, max = theta)
  ks_results[[as.character(theta)]] <- list(
    theta = theta,
    D_statistic = round(ks_test_result$statistic, 3),
    p_value = formatC(ks_test_result$p.value, format = "e", digits = 3)
  )
}
```

```{r, echo=FALSE, results='asis'}
for (res in ks_results) {
  cat("**Kolmogorov-Smirnov Test for Uniform(3,", res$theta, "):**\n")
  cat("D-statistic:", res$D_statistic, ", P-value:", res$p_value, "\n")
}
```

For theta = 9, theta = 10, and theta = 11 we get a p-value \< 0.05, thus we reject H0 indicating that the observed maximum for these values is significantly different from the expected maximum under U(3,theta). For theta = 8, we fail to reject H0, meaning that the data could be from a uniform distribution.

#### Section e

To test whether the median of the cholesterol level is less than 6, we will use a Wilcoxon signed-rank test, because it is a non-parametric test that does not assume normality, making it suitable for small sample sizes and skewed data. The null hypothesis is:

H0: The median cholesterol level is 6 or greater.

The test yields a p-value of 0.223, meaning we fail to reject H0​. This suggests that the median cholesterol level after 8 weeks is not significantly less than 6 at the 5% significance level.

```{r, results='asis'}
wilcox_result = wilcox.test(data$After8weeks, mu = 6, alternative = "less", exact = FALSE)
```

```{r, echo=FALSE, results='asis'}
rounded_V = round(wilcox_result$statistic, 3)
rounded_p_value = formatC(wilcox_result$p.value, format = "e", digits = 3)
cat("**Wilcoxon Signed-Rank Test:**", "V-statistic:", rounded_V, ", p-value:", rounded_p_value, "\n")

```

To determine whether the fraction of cholesterol levels below 4.5 after 8 weeks exceeds 25%, a binomial test is conducted. The null hypothesis is:

H0: The fraction of cholesterol levels below 4.5 is at most 25%.

The test yields a p-value of 0.865, meaning we fail to reject H0​ at the 5% significance level. This suggests that there is no significant evidence that the proportion of cholesterol levels below 4.5 is greater than 25%.

```{r}
count_below_4.5 = sum(data$After8weeks < 4.5)
binom_result = binom.test(count_below_4.5, length(data$After8weeks), p = 0.25, alternative = "greater")
```

```{r, echo=FALSE, results='asis'}
rounded_p_value = round(binom_result$p.value, 3)
rounded_conf_int = round(binom_result$conf.int, 3)
rounded_prob_estimate = round(binom_result$estimate, 3)

# Print formatted results
cat("**Exact binomial test**\n\n")
cat("Number of successes:", count_below_4.5, ", Number of trials:", length(data$After8weeks), ", p-value:", rounded_p_value, ", 95% CI: [", rounded_conf_int, "], Estimated probability of success:", rounded_prob_estimate, "\n")
```

## Exercise 2: Crops

```{r, echo=FALSE}

  crops_data <- read.table("crops.txt", header=TRUE)
  crops_data$County <- as.factor(crops_data$County)         
  crops_data$Related <- as.factor(crops_data$Related)
  
```

#### Section a

We want to investigate whether two factors County and Related (and possibly their interaction) influence the crops by performing relevant ANOVA model(s), without taking Size into account. So we create and test 3 separate Null Hypotheses with a two-way ANOVA and a one-way ANOVA on the additive model: H\_(01): no main effect of factor County, H\_(02): no main effect of factor Related and H\_(03): no interactions between factors County and Related

```{r, echo=FALSE}
model_a <- lm(Crops ~ County * Related, data = crops_data)
anova(model_a)
```

From this table we can see the following:

For County: The County p-value \> 0.05, meaning we fail to reject the null hypothesis H\_(01), suggesting that there is no significant effect of the County on the Crops variable.

For Related: The Related p-value \> 0.05, meaning we fail to reject the null hypothesis H\_(02), which suggests that there is no significant effect of whether the landlord and tenant are related on the Crops variable.

For both: The Interaction p-value \> 0.05, meaning we fail to reject the null hypothesis H\_(03), implying there is no significant interaction between County and Related on the Crops variable.

```{r, echo=FALSE}
model_summary <- summary(model_a)
cat("These are the coefficients:\n")
print(model_summary$coefficients)
```

The above model summary table aligns with the ANOVA p-values as both show that none of the predictors(County, Related, or their interaction) are significant in either table.

```{r, echo=FALSE, fig.width = 8, fig.height = 4}
# Set up a 1-row, 2-column layout
par(mfrow = c(1, 2))

# 1st interaction plot: County & Related affecting Crops
interaction.plot(x.factor = crops_data$County, 
                 trace.factor = crops_data$Related, 
                 response = crops_data$Crops,
                 main = "Interaction: County & Related",
                 xlab = "County", ylab = "Crops", 
                 trace.label = "Related", col = c("blue", "red"), lty = 1, pch = 19)

# 2nd interaction plot: Related & County affecting Crops (switched roles)
interaction.plot(x.factor = crops_data$Related, 
                 trace.factor = crops_data$County, 
                 response = crops_data$Crops,
                 main = "Interaction: Related & County",
                 xlab = "Related", ylab = "Crops", 
                 trace.label = "County", col = 1:3, lty = 1, pch = 19)

```

In the above interaction plots the lines seem parallel, therefore interaction seems to not be present, verifying the two-way anova results.

```{r, echo=FALSE, fig.width = 6, fig.height = 3}
par(mfrow = c(1, 2))  # Set layout for two side-by-side plots

# Boxplot for County main effect
boxplot(Crops ~ County, data = crops_data,
        col = "lightblue",
        main = "Effect of County on Crops",
        xlab = "County",
        ylab = "Crops")

# Boxplot for Related main effect
boxplot(Crops ~ Related, data = crops_data,
        col = "lightgreen",
        main = "Effect of Related on Crops",
        xlab = "Related",
        ylab = "Crops")


```

ANOVA shows no significant effect of County or Related on crop yield. High p-values suggest no strong differences, consistent with the boxplot, where distributions overlap, medians are close, and no outliers appear.

Finally, we have to check the model assumptions for normality

```{r, echo=FALSE}
p_value <- round(shapiro.test(residuals(model_a))$p.value, 3)
statistic <- round(shapiro.test(residuals(model_a))$statistic, 3)
print(sprintf("Shapiro-Wilk test for model_a: p = %.3f, W = %.3f", p_value, statistic))
```

The Q-Q plot shows deviations from normality, particularly in the tails, but the overall trend follows the theoretical quantiles. The residuals vs. fitted plot suggests no strong patterns, indicating an approximately random distribution of residuals. The Shapiro-Wilk test fails to reject the null hypothesis of normality at the 0.05 level. Given the small sample size (n = 30), results should be interpreted with caution, as minor departures from normality can impact statistical inference.

```{r, echo=FALSE, fig.width = 6, fig.height = 3 }
par(mfrow=c(1,2))
qqnorm(residuals(model_a))
qqline(residuals(model_a), col = "red", lwd = 2) 
plot(fitted(model_a), residuals(model_a))

```

To estimate crop yields for County 3 when the landlord and tenant are unrelated, we use the emmeans function to calculate the adjusted mean yield. This estimation is based on model_a, which incorporates the County-Size interaction. The emmeans function provides the estimated marginal means, accounting for the effects of County and Related while adjusting for interactions.

```{r, echo=FALSE}
emm_results <- emmeans::emmeans(model_a, ~ County * Related)
emm_summary <- as.data.frame(emm_results)
county3_not_related <- emm_summary[emm_summary$County == "3" & 
emm_summary$Related == "no", ]
cat("Estimated crops for County 3 (Landlord and Tenant NOT related):", county3_not_related$emmean, "\n")

```

#### Section b

```{r, echo=FALSE}
crops_data$County <- as.factor(crops_data$County)
crops_data$Related <- as.factor(crops_data$Related)
crops_data$Size <- as.numeric(crops_data$Size) 
crops_data$Crops <- as.numeric(crops_data$Crops)
```

We define 3 different models:

1.  Model_county_size: Examines the effects of County, Related, and Size on crop yield, focusing on the County × Size interaction but excluding Related interactions.

```{r, echo=FALSE}
model_county_size <- lm(Crops ~ Size * County + Related, data = crops_data)
anova(model_county_size)
```

2.  Model_related_size: Evaluates the effects of County, Related, and Size on crop yield, adding a Related × Size interaction to test if Size’s effect depends on landlord-tenant relation.

```{r, echo=FALSE}
model_related_size <- lm(Crops ~ Size * Related + County, data = crops_data)
anova(model_related_size)
```

3.  Model_additive: Assumes each factor affects crop yield independently, without testing interactions—only the individual effects of County, Related, and Size.

```{r, echo=FALSE}
model_additive <- lm(Crops ~ Size + County + Related, data = crops_data)
anova(model_additive)
```

We have tested interaction models as well as purely additive. The interaction Size-Related and the individual effect of County and Related are insignificant(p-values\>0.5). Therefore, the best model is model_county_size, since it shows the significance of Size and of the interaction Size-County.

Finally, we can check this model's assumptions.

```{r, echo=FALSE, fig.width = 6, fig.height = 3}
par(mfrow=c(1,2))
qqnorm(residuals(model_county_size))
qqline(residuals(model_county_size), col = "red", lwd = 2) 
plot(fitted(model_county_size), residuals(model_county_size))
```

```{r, echo=FALSE}
p_value <- round(shapiro.test(residuals(model_county_size))$p.value, 3)
statistic <- round(shapiro.test(residuals(model_county_size))$statistic, 3)

print(sprintf("Shapiro-Wilk test for model_county_size: p = %.3f, W = %.3f", p_value, statistic))
```

The Shapiro-Wilk test for model_county_size residuals suggests no significant deviation from normality, supported by the QQ-plot’s linear pattern.

#### Section c

```{r, echo=FALSE}
model_summary <- summary(model_county_size)
cat("These are the coefficients:\n")
print(model_summary$coefficients) # Extracts only the coefficients
cat("\nThis is the R-squared value:", model_summary$r.squared, "\n")
```

The coefficient for Size is 22.704 (p\<0.001), meaning that in County 1 (the reference level), each unit increase in Size leads to an expected 22.7-unit increase in Crops. This effect is statistically significant, confirming a strong positive influence. County 2 has a negative coefficient of -4214.050 (p\<0.01), meaning crop yields there are 4214 units lower than in County 1 for the same Size. County 3’s coefficient (-1284.813, p=0.334) is not statistically significant, so we cannot conclude a strong difference from County 1. Related has a coefficient of -239.099 (p=0.499), indicating no significant impact on Crops. The Size:County2 interaction is 26.590 (p=0.003), meaning that the effect of Size on Crops in County 2 is stronger than in County 1, with a total increase of 49.3 units per Size unit. The Size:County3 interaction (8.916, p=0.176) is not statistically significant, so we cannot confidently conclude a difference from County 1. The model explains 86.6% of the variation in Crops (R²=0.866), indicating strong explanatory power. 

```{r, echo=FALSE}
confint(model_county_size)
```

The above confidence intervals confirm that Size has a strong, statistically significant positive effect, and that County 2 and the Size:County2 interaction also have significant effects. In contrast, the confidence intervals for County 3, Related, and Size:County3 include zero, meaning there is no strong evidence that these factors significantly influence Crops.

#### Section d

```{r, echo=FALSE}
pred <- emmeans::emmeans(model_county_size, specs = ~ County * Related * Size, 
                at = list(County = "2", Size = 165, Related = "yes"))

summary(pred)
```

The predicted yield crops for a farm from County 2 of size 165, with related landlord and tenant is 6141, with a 95% CI: (5428, 6855)

```{r, echo=FALSE}
pred_se <- summary(pred)$SE
# Compute the prediction variance
pred_var <- pred_se^2
# Print results
cat("Prediction Variance:", pred_var, "\n")
cat("Residual Variance (sigma^2):", sigma(model_county_size)^2, "\n")
cat("Total Variance:",pred_var + sigma(model_county_size)^2)
```

The fact that the prediction variance is much smaller than the residual variance suggests that most of the uncertainty is due to the residual variation (random noise or factors not captured by the model), rather than the instability of the model’s coefficient estimates.

# Exercise 3: Yield of peas

```{r include=FALSE}
library(MASS)
library(ggplot2)
library(dplyr)
data(npk)
```

#### Section a

To optimize possible future instances of a random plot distribution process we create a data frame similar to that of the npk data such that it may be utilized similarly.

```{r}
# Set block and plot dimensions
n_blocks <- 6
n_plots <- 4

# Create a data frame to randomly distribute over blocks
random_distributed <- data.frame(block = rep(1:n_blocks, each = n_plots),
                                 N = rep(0, n_plots*n_blocks),
                                 P = rep(0, n_plots*n_blocks),
                                 K = rep(0, n_plots*n_blocks))

# Iterate over blocks for index and sampling
for (block in 1:n_blocks) {
  
  idx <- (n_plots * (block-1) + 1): (n_plots * block)
  
  random_distributed[sample(idx, 2), "N"] <- 1
  random_distributed[sample(idx, 2), "P"] <- 1
  random_distributed[sample(idx, 2), "K"] <- 1
}
```

#### Section b

From the npk data a bar graph is generated to report the average *yield* per *block* in the presence (1) and absence (0) of *N*.

```{r, echo=FALSE, fig.width = 4, fig.height = 2}
# Group npk data by block and N
grouped_data <- group_by(npk, block, N)

# Calculate the average yield for each group
average_yield_block <- summarise(grouped_data, Average_Yield = mean(yield), .groups = "keep")

# Plot the average yield
avg_yield_plot <- ggplot(average_yield_block, 
    aes(x = factor(block), y = Average_Yield, fill = factor(N))) + 
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Block Number", y = "Average Yield", fill = "Nitrogen") +
  ggtitle("Average Yield per Block, With and Without Nitrogen")

avg_yield_plot

```

From the graph we can note that there seems to be a correlation between a higher *yield* and the presence of the additive *N* in the plot soil. We will further investigate this hypothesis with different tests such as a full two-way ANOVA. The *block* factor is not of interest but is applied to group plot tests and may introduce variability within the samples.

#### Section c

A full two-way ANOVA is conducted with the response variable *yield* and the two factors *block* and *N*. We start with a main effects model.

```{r, echo=FALSE}
two_way_anova_main <- aov(yield ~ block + N, data = npk)
summary(two_way_anova_main)
```

We then compute an interaction model.

```{r, echo=FALSE}
two_way_anova_int <- aov(yield ~ block * N, data = npk)
summary(two_way_anova_int)
```

With a p-value of 0.0071 and 0.0262 respectively in the main effects model, the ANOVA signifies that both the *N* additive and *block* factor have statistically significant effects on the yield of peas with nitrogen having a larger effect compared to the block. However, we might say that the *block* factor merely introduces variability in the samples and do not represent fixed system effects. Including the *block* factor is, however, important to understand the amount of variability it may cause. In the interaction model both factors again have a p-value below 0.05. Additionally, we notice that there is no statistically significant effect on the *yield* from an interaction between *N* and *yield*.

To solidify our findings we perform an analysis on the residuals of the main effects model with a normality test and a plot of the distribution with a histogram.

```{r, echo=FALSE, fig.width = 6, fig.height = 3}
par(mfrow=c(1,2))
# Analysis of residuals (normality test)
qqnorm((residuals)(two_way_anova_main))
qqline(residuals(two_way_anova_main), col = 'red')

# Distribution in histogram (since slight deviation at tail)
hist(residuals(two_way_anova_main), main = "Histogram of Residuals", xlab = "Residuals", breaks = 10)

```

We note that there is a slight deviation at the tail of the normality test which we investigate this further with a Shapiro Wilk test.

```{r, echo=FALSE}
# Shapiro Wilk test (Since slight deviation at tail)
shapiro.test(residuals(two_way_anova_main))
```

From the Shapiro-Wilk test we receive a p-value greater than 0.05, combined with the Q-Q and histogram it is likely normally distributed.

Tests indicate a normal distribution of the residuals and the data is continuous. We can therefore say that a non-parametric designed test such as a Friedman test is not applicable. Additionally, We have limited in-block variation due to a randomized block design with continuous data which is more applicable to the ANOVA model design.

#### Section d

To investigate the effects of other variables and any other possible interactions we explore more models. We first test for the presence of additional main effects from all additives and the block factor.

```{r, echo=FALSE}
# Main effect model of all factors
model_1 <- aov(yield ~ N + P + K + block, data = npk)
summary(model_1)
```

We now learn that potassium has a statistically significant effect on the yield with a p-value of 0.02767. From this main effects model we therefore learn that only *P* does not contribute significantly, we will therefore omit *P* in subsequent models.

Next, we explore the interaction between potassium and the other remaining main effect variables *N* and *block*.

```{r, echo=FALSE}
# Interaction model between N, K and block
model_2 <- aov(yield ~ N * K + K * block, data = npk)
summary(model_2)
```

From the data we learn that there is no significant interaction between the different factors. We further explore this with an interaction plot.

```{r, echo=FALSE, fig.width = 6, fig.height = 3}
par(mfrow=c(1,2))
# Check to see if no interaction between variables
interaction.plot(npk$N, npk$block, npk$yield,
                 xlab = "Nitrogen (N)", 
                 ylab = "Yield", 
                 trace.label = "Block")
interaction.plot(npk$block, npk$N, npk$yield,
                 xlab = "Block", 
                 ylab = "Nitrogen (N)", 
                 trace.label = "Yield")
```

From this plot we only observe a minor possible interaction between *block* and *N*. However, due to the overall linearity we suspect the interaction to be mostly negligible.

We will therefore approach a main effects model where we leave out phosphor to omit arbitrary data and prevent overfitting. The main effect model will include factors *N*, *K* and *block*

```{r}
# NOTE: P could be omitted due to statistical insignificance
model_3 <- aov(yield ~ N + K + block, data = npk)
summary(model_3)
```

#### Section e

To investigate the influence of the factors on the *yield*, we will perform a Tukey's Honest Significant Difference (HSD) test.

```{r}
# Apply TukeyHSD 
tukey_results <- TukeyHSD(model_3)
tukey_results
```

From the TukeyHSD results we see that K has a significant negative impact on yield. Therefore we can say that the best combination for model 3 (where we do not consider *P*) is *N* = 1 and *K* = 0. Additionally, the *block* factor introduces variability with *block* 3 having the overall biggest positive impact and *block* 1 a relatively lower *yield*.

#### Section f

Finally, we create a mixed effects analysis for our chosen main effect model with factors *N*, *K* and *block* where we model *block* as a random effect.

```{r, echo=FALSE}
# Create a Mixed effects model with all chosen factors.
mixed_effects_model <- lme4::lmer(yield ~ N + K + (1 | block), data = npk)
summary(mixed_effects_model)
```

Similarly to the additive model, we can conclude that *N* has a significant (5.617) positive effect on the *yield* with a p-value of 0.00302, whereas *K* has a statistically significant negative effect on the *yield* (-3.983) with a p-value of 0.02487. This analysis shows that *blocks* have significant variability in data, with a random effects variance of 13.16. This mixed effects model further solidifies the stance that *N* has a significant impact on *yield*. We further note that the mixed effects model may provide a better representation, since *blocks* introduce random variance and not fixed systemic effects such as *N*, *P* and *K*.
:::
