---
title: "Assignment 1 - Report"
author: "Name, Eleni Liarou, Zoë Azra Blei, Frederieke Loth, group 20"
date: "23 February 2025"
output:
  pdf_document:
    latex_engine: xelatex
fontsize: 11pt
highlight: tango
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
install.packages("tinytex", repos = "https://cran.r-project.org")
```

## Exercise 1

First we load and read the necessary data set

```{r}
data = read.delim("cholesterol.txt", sep=' ')
```

#### a) Make some relevant plots of this data set, comment on normality. Investigate whether the columns *Before* and *After8weeks* are correlated.

In order to investigate the normality of the data set, Q-Q plots are
created below for both the *Before* and *After8weeks* columns. As in
both plots the data points closely follow the diagonal red line, the
data is approximating a normal distribution. While some minor deviations
may be present in the tails, the overall pattern suggests that the
normality assumption is reasonable.

```{r, echo=FALSE}
par(mfrow = c(1,2)) 

qqnorm(data$Before, main = "Q-Q Plot for Before")
qqline(data$Before, col = "red")

qqnorm(data$After8weeks, main = "Q-Q Plot for After 8 Weeks")
qqline(data$After8weeks, col = "red")

```

To further explore the normality assumption, histograms below were
plotted for both 'Before' and 'After8weeks'. The histograms exhibit a
roughly bell-shaped distribution, which supports the assumption of
normality.

```{r, echo=FALSE}
par(mfrow = c(1,2))  

hist(data$Before, probability = TRUE, 
     main = 'Distribution for Cholesterol \n level Before margarine',
     xlab = 'Cholesterol level (mmol/L)',
     ylab = 'Probability')
lines(density(data$Before), col = "red", lwd = 2)

hist(data$After8weeks, probability = TRUE, 
     main = 'Distribution for Cholesterol \n level After margarine',
     xlab = 'Cholesterol level (mmol/L)',
     ylab = 'Probability')
lines(density(data$After8weeks), col = "red", lwd = 2)
```

However, to address normality more formally, a Shapiro-Wilk test is
conducted, as this test is suitable to test on normallity for small data
sets. For the test the null hypothesis is as follows:

H0: The data is normally distributed.

The W-statistic measures how closely the data aligns with a normal
distribution, ranging from 0 to 1, where values closer to 1 indicate a
stronger likelihood of normality. Considering the results for *Before*
and *After8weeks*, both W-values are close to 1. Additionally, with a
95% confidence level, both p-values exceed 0.05, meaning that we fail to
reject H0. These findings provide strong evidence that the data in both
columns can be considered to be normally distributed.

```{r}
shapiro.test(data$Before)
shapiro.test(data$After8weeks)
```

In order to investigate the relationship between the columns of *Before*
and *After8weeks* a scatter plot is created below. The scatter plot
demonstrates a strong positive correlation between 'Before' and
'After8weeks' cholesterol levels. The data points align closely with the
red regression line, suggesting that individuals with higher cholesterol
levels before the diet intervention also tend to have higher cholesterol
levels after 8 weeks. This indicates that while cholesterol levels may
have decreased, there remains a strong relationship between pre- and
post-diet measurements.

```{r, echo=FALSE}
plot(data$Before, data$After8weeks, 
     main = 'Regression for Before and After8weeks', 
     xlab='Before', 
     ylab='After')
abline(lm(After8weeks ~ Before, data = data), col = "red")
```

To quantify this correlation, the Pearson’s correlation coefficient is
calculated below. A high Pearson correlation (close to 1) indicates a
strong positive relationship between the two columns. The correlation
coefficient exhibits a value of approximately 0.99, confirming the
strong positive relationship. Additionally, the p-value is smaller than
0.05, indicating that the correlation is statistically significant.

```{r}
cor.test(data$Before, data$After8weeks, method = "pearson")
```

#### b) Apply a couple of relevant tests (at least two tests, see Lectures 2–3) to verify whether the diet with low fat margarine has an eﬀect (argue whether the data are paired or not). Is a permutation test applicable? Is the Mann-Whitney test applicable?

As the cholesterol data was measured on the same population at different
times, we consider the data to be paired. In this case it is possible to
conduct a T-test for paired samples. However, in order to This test
assumes that the mean difference of the two populations is normally
distributed, thus, a Shapiro-Wilk test is conducted first to investigate
the distribution.

```{r}
difference = data$Before - data$After8weeks
shapiro.test(difference)
```

Explain that the data is paired:

We are doing a T-test

The null hypothesis is as follows:

H0: The margarine diet has no effect, i.e. the mean cholesterol levels
*Before* and *After8weeks* are the same.

```{r}
# H0: The margarine diet has no effect, i.e. the mean cholesterol levels Before and After 8 weeks are the same
# H1: The margarine diet reduces cholesterol levels --> mean_before > mean_after
# Paired t-test
# Assumption: the differences between Before and After should be normally distributed
# Outcome: if p < 0.05, reject H0
t.test(data$Before, data$After8weeks, paired = TRUE, alternative = "greater")

# Visualizing distribution of the differences
difference = data$Before - data$After8weeks

shapiro.test(difference)

hist(difference, probability = TRUE,
     main = 'Data distribution of differences between Before and After8weeks',
     col = 'lightblue',
     xlab = 'Difference (Before - After8weeks)',
     ylab = 'Proportion')
lines(density(difference), col = 'red', lwd = 2)

qqnorm(difference,
       main = 'QQ-plot for differences',
       xlab = 'Theoretical Quantiles',
       ylab = 'Sample Quantiles')
qqline(difference, col = "red")
```

```{r}


# Doing permutation test:
# H0: there is no difference between the Before and After8weeks groups
diff = data$Before - data$After8weeks
n_permutations = 1000
observed_mean = mean(diff)

permute_test = function(diff) {
  permuted_diff <- diff * sample(c(-1, 1), length(diff), replace = TRUE)  # Randomly flip signs
  return(mean(permuted_diff)) 
}

set.seed(42)
permute_distr = replicate(n_permutations, permute_test(diff))

# Histogram of the permutation distribution
hist(permute_distr, probability = TRUE, col = "gray", 
     main = "Permutation Test Distribution", xlab = "Mean Differences",
     xlim = range(c(permute_distr, observed_mean)))  # Adjust x-axis limits

# Add a red line at observed mean difference
abline(v = observed_mean, col = "red", lwd = 2, lty = 2)

# Print observed mean to check if it is within range
cat("Observed mean difference:", observed_mean, "\n")

# Compute p-value
p_value = mean(abs(permute_distr) >= abs(observed_mean))
cat("Permutation test p-value:", p_value, "\n")

# Doing a Mann-Whitney U-test, as we are testing ordinal data
# H0: There is no difference in mean cholesterol level in the Before and After groups
wilcox.test(data$Before, data$After8weeks, paired = TRUE, alternative = "two.sided")
# H1: cholesterol levels are lower after 8 weeks
wilcox.test(data$Before, data$After8weeks, paired = TRUE, alternative = "greater")
```

#### c) Let *X1,...,X18* be the column *After8weeks*. Assume *X1,...,X18 \~ N(mu, sigma\^2) (*irrespective of your conclusion in a)) with unknown μ and σ\^2. Construct a 97%-CI for μ based on normality. Next, construct a bootstrap 97%-CI for μ and compare it to the above CI.

```{r}
# calculating 97% CI for mu using t-score
n = length(data$After8weeks)
sample_mean = mean(data$After8weeks)
sample_sd = sd(data$After8weeks)
critical_value = qt(1-0.015, df=17)
standard_error = sample_sd / sqrt(n)

left_bound = sample_mean - critical_value * standard_error
right_bound = sample_mean + critical_value * standard_error

cat("97% Confidence Interval for mu: [", left_bound, ",", right_bound, "]\n")

# calculating 97% CI for mu with bootstrapping
bootstrap_ci = function(x, conf_level = 0.97, B = 10000) {
  alpha = 1 - conf_level
  Bstats = lapply(1:B, FUN = function(i) {
    boot_sample = sample(x, size = length(x), replace = TRUE)
    mean(boot_sample)
  } )
  Bstats = unlist(Bstats)
  quantile(Bstats, prob = c(alpha/2, 1-alpha/2))
}

set.seed(42)
bootstrap_ci(data$After8weeks)
```

#### d) Using a bootstrap test with test statistic T=max(X1,…,X18), determine those θ∈[3,12] (if there are any) for which  H0:X1,…,X18∼Unif[3,θ] is not rejected. Can the Kolmogorov-Smirnov test be also applied for this question? If yes, apply it; if not, explain why not.

#### e) Using an appropriate test, verify whether the median cholesterol level after 8 weeks of low fat diet is less than 6. Next, design and perform a test to check whether the fraction of the cholesterol levels after 8 weeks of low fat diet less than 4.5 is at most 25%.

```{r}
median(data$After8weeks)
wilcox.test(data$After8weeks, mu = 6, alternative = "less")

# Count how many values in After8weeks are less than 4.5
count_below_4.5 = sum(data$After8weeks < 4.5)
percentage_below_4.5 = (count_below_4.5 / length(data$After8weeks)) * 100
cat("Percentage of cholesterol levels below 4.5:", percentage_below_4.5, "%\n")

# H0: The fraction of cholesterol levels below 4.5 is at most 25%
# H1: The fraction is greater than 25%
# if the p-value is small (<0.05), we reject H0 and conclude that the fraction is significantly greater than 25%.
binom.test(count_below_4.5, length(data$After8weeks), p = 0.25, alternative = "greater")
```

## Exercise 2: Crops

```{r, echo=FALSE}

  crops_data <- read.table("crops.txt", header=TRUE)
  crops_data$County <- as.factor(crops_data$County)         
  crops_data$Related <- as.factor(crops_data$Related)
  
```

### Section a

We want to investigate whether two factors County and Related (and
possibly their interaction) influence the crops by performing relevant
ANOVA model(s), without taking Size into account. So we create and test
3 separate Null Hypotheses with a two-way ANOVA and a one-way ANOVA on
the additive model:

-   H\_(01): no main effect of factor County

-   H\_(02): no main effect of factor Related

-   H\_(03): no interactions between factors County and Related

```{r}
model_a <- lm(Crops ~ County * Related, data = crops_data)
anova(model_a)
```

From this table we can see the following:

For County: The p-value (0.476) is greater than 0.05, meaning we fail to
reject the null hypothesis H\_(01), suggesting that there is no
significant effect of the County on the Crops variable.

For Related: The p-value (0.527) is also greater than 0.05, meaning we
fail to reject the null hypothesis H\_(02), which suggests that there is
no significant effect of whether the landlord and tenant are related on
the Crops variable.

For both: The p-value (0.879) is much greater than 0.05, meaning we fail
to reject the null hypothesis H\_(03), implying there is no significant
interaction between County and Related on the Crops variable.

```{r}
summary(model_a)
```

The above model summary table aligns with the ANOVA p-values as both
show that none of the predictors(County, Related, or their interaction)
are significant in either table.

```{r, echo=FALSE, fig.width = 8, fig.height = 4}
# Set up a 1-row, 2-column layout
par(mfrow = c(1, 2))

# 1st interaction plot: County & Related affecting Crops
interaction.plot(x.factor = crops_data$County, 
                 trace.factor = crops_data$Related, 
                 response = crops_data$Crops,
                 main = "Interaction: County & Related",
                 xlab = "County", ylab = "Crops", 
                 trace.label = "Related", col = c("blue", "red"), lty = 1, pch = 19)

# 2nd interaction plot: Related & County affecting Crops (switched roles)
interaction.plot(x.factor = crops_data$Related, 
                 trace.factor = crops_data$County, 
                 response = crops_data$Crops,
                 main = "Interaction: Related & County",
                 xlab = "Related", ylab = "Crops", 
                 trace.label = "County", col = 1:3, lty = 1, pch = 19)

```

In the above interaction plots the lines seem parallel, therefore
interaction seems to not be present, verifying the two-way anova
results.

```{r, echo=FALSE, fig.width = 6, fig.height = 3}
par(mfrow = c(1, 2))  # Set layout for two side-by-side plots

# Boxplot for County main effect
boxplot(Crops ~ County, data = crops_data,
        col = "lightblue",
        main = "Effect of County on Crops",
        xlab = "County",
        ylab = "Crops")

# Boxplot for Related main effect
boxplot(Crops ~ Related, data = crops_data,
        col = "lightgreen",
        main = "Effect of Related on Crops",
        xlab = "Related",
        ylab = "Crops")


```

ANOVA shows no significant effect of County or Related on crop yield.
High p-values suggest no strong differences, consistent with the
boxplot, where distributions overlap, medians are close, and no outliers
appear.

Finally, we have to check the model assumptions

```{r, echo=FALSE, fig.width = 6, fig.height = 3 }
par(mfrow=c(1,2))
qqnorm(residuals(model_a))
qqline(residuals(model_a), col = "red", lwd = 2) 
plot(fitted(model_a), residuals(model_a))


```

```{r, echo=FALSE}
p_value <- round(shapiro.test(residuals(model_a))$p.value, 3)
statistic <- round(shapiro.test(residuals(model_a))$statistic, 3)
print(sprintf("These are the p-values of the Shapiro-Wilk test for model_a: p = %.3f, W = %.3f", p_value, statistic))
```

The Q-Q plot shows deviations from normality, particularly in the tails,
but the overall trend follows the theoretical quantiles. The residuals
vs. fitted plot suggests no strong patterns, indicating an approximately
random distribution of residuals. The Shapiro-Wilk test (W = 0.941, p =
0.099) fails to reject the null hypothesis of normality at the 0.05
level. Given the small sample size (n = 30), results should be
interpreted with caution, as minor departures from normality can impact
statistical inference.

To estimate crop yields for County 3 when the landlord and tenant are
unrelated, we use the emmeans function to calculate the adjusted mean
yield. This estimation is based on model_a, which incorporates the
County-Size interaction. The emmeans function provides the estimated
marginal means, accounting for the effects of County and Related while
adjusting for interactions.

```{r, echo=FALSE}
emm_results <- emmeans::emmeans(model_a, ~ County * Related)
emm_summary <- as.data.frame(emm_results)
county3_not_related <- emm_summary[emm_summary$County == "3" & 
emm_summary$Related == "no", ]
cat("Estimated crops for County 3 (Landlord and Tenant NOT related):", county3_not_related$emmean, "\n")

```

### Section b

```{r, echo=FALSE}
crops_data$County <- as.factor(crops_data$County)
crops_data$Related <- as.factor(crops_data$Related)
crops_data$Size <- as.numeric(crops_data$Size) 
crops_data$Crops <- as.numeric(crops_data$Crops)
```

We define 3 different models:

1.  Model_county_size: This model examines how crop yields are
    influenced by County, Related, and Size, with an additional focus on
    the interaction between County and Size. It does not include an
    interaction term for Related.

```{r}
model_county_size <- lm(Crops ~ Size * County + Related, data = crops_data)
anova(model_county_size)
```

2.  Model_related_size: This model evaluates how County, Related, and
    Size affect crop yields, and it adds an interaction term for Related
    and Size to test if the effect of Size on crop yields depends on
    whether the landlord and tenant are related.

```{r}
model_related_size <- lm(Crops ~ Size * Related + County, data = crops_data)
anova(model_related_size)
```

3.  Model_additive: This model assumes that the effect of each factor on
    crop yield is independent of the others. It does not test for any
    interaction effects, only the individual contributions of County,
    Related, and Size to crop yields.

```{r}
model_additive <- lm(Crops ~ Size + County + Related, data = crops_data)
anova(model_additive)
```

We have tested interaction models as well as purely additive. The
interaction Size-Related and the individual effect of County and Related
are insignificant(all 3 have p-values\>0.5). Therefore, the best model
is model_county_size, since it shows the significance of Size and of the
interaction Size-County.

Finally, we can check this model's assumptions.

```{r, echo=FALSE, fig.width = 6, fig.height = 3}
par(mfrow=c(1,2))
qqnorm(residuals(model_county_size))
qqline(residuals(model_county_size), col = "red", lwd = 2) 
plot(fitted(model_county_size), residuals(model_county_size))
```

```{r, echo=FALSE}
p_value <- round(shapiro.test(residuals(model_county_size))$p.value, 3)
statistic <- round(shapiro.test(residuals(model_county_size))$statistic, 3)

print(sprintf("These are the p-values of the Shapiro-Wilk test for model_county_size: p = %.3f, W = %.3f", p_value, statistic))
```

The Shapiro-Wilk test for model_county_size residuals ( p = 0.733 )
suggests no significant deviation from normality, supported by the
QQ-plot’s linear pattern.

### Section c

```{r}
summary(model_county_size)
```

The coefficient for Size is 22.704 (p\<0.001), meaning that in County 1
(the reference level), each unit increase in Size leads to an expected
22.7-unit increase in Crops. This effect is statistically significant,
confirming a strong positive influence. County 2 has a negative
coefficient of -4214.050 (p\<0.01), meaning crop yields there are 4214
units lower than in County 1 for the same Size. County 3’s coefficient
(-1284.813, p=0.334) is not statistically significant, so we cannot
conclude a strong difference from County 1. Related has a coefficient of
-239.099 (p=0.499), indicating no significant impact on Crops. The
Size:County2 interaction is 26.590 (p=0.003), meaning that the effect of
Size on Crops in County 2 is stronger than in County 1, with a total
increase of 49.3 units per Size unit. The Size:County3 interaction
(8.916, p=0.176) is not statistically significant, so we cannot
confidently conclude a difference from County 1. The model explains
86.6% of the variation in Crops (R²=0.866), indicating strong
explanatory power. 

```{r}
confint(model_county_size)
```

The confidence intervals confirm that Size has a strong, statistically
significant positive effect, and that County 2 and the Size:County2
interaction also have significant effects. In contrast, the confidence
intervals for County 3, Related, and Size:County3 include zero, meaning
there is no strong evidence that these factors significantly influence
Crops.

### Section d

```{r}
pred <- emmeans::emmeans(model_county_size, specs = ~ County * Related * Size, 
                at = list(County = "2", Size = 165, Related = "yes"))

summary(pred)
```

The predicted yield crops for a farm from County 2 of size 165, with
related landlord and tenant is 6141, with a 95% CI: (5428, 6855)

```{r, echo=FALSE}
pred_se <- summary(pred)$SE
# Compute the prediction variance
pred_var <- pred_se^2
# Print results
cat("Prediction Variance:", pred_var, "\n")
cat("Residual Variance (sigma^2):", sigma(model_county_size)^2, "\n")
cat("Total Variance:",pred_var + sigma(model_county_size)^2)
```

The fact that the prediction variance is much smaller than the residual
variance suggests that most of the uncertainty is due to the residual
variation (random noise or factors not captured by the model), rather
than the instability of the model’s coefficient estimates.

# Exercise 3
  
Here, we will explore the effect of different additives on the yield of peas with a primary focus on nitrogen (*N*). Other additives are potassium (*K*) and phosphorus (*P*). The data is obtained through the MASS package library (npk) and represents 6 blocks of soil, each containing 4 plots. Each *block* contains exactly two of each additive and no plot can have two of the same additive.

We start by loading the neccesary npk dataset from the MASS package and other packages.
    
```{r, echo=FALSE}
library(MASS)
data(npk)
```

### Section a
    
To optimize possible future instances of a random plot distribution process we create a data frame similar to that of the npk data such that it may be utilized similarly.
    
    ```{r}
    # Set block and plot dimensions
    n_blocks <- 6
    n_plots <- 4

    # Create a data frame to randomly distribute over blocks
    random_distributed <- data.frame(block = rep(1:n_blocks, each = n_plots),
                                     N = rep(0, n_plots*n_blocks),
                                     P = rep(0, n_plots*n_blocks),
                                     K = rep(0, n_plots*n_blocks))

    # Iterate over blocks for index and sampling
    for (block in 1:n_blocks) {
  
      idx <- (n_plots * (block-1) + 1): (n_plots * block)
  
      random_distributed[sample(idx, 2), "N"] <- 1
      random_distributed[sample(idx, 2), "P"] <- 1
      random_distributed[sample(idx, 2), "K"] <- 1
    }
    ```

### Section b 
From the npk data a bar graph is generated to report the average *yield* per *block* in the presence (1) and absence (0) of *N*.
       
    ```{r, echo=FALSE, fig.width = 6, fig.height = 4}
    # Group npk data by block and N
    grouped_data <- group_by(npk, block, N)
    
    # Calculate the average yield for each group
    average_yield_block <- summarise(grouped_data, Average_Yield = mean(yield), .groups = "keep")
    
    # Plot the average yield
    avg_yield_plot <- ggplot(average_yield_block, 
        aes(x = factor(block), y = Average_Yield, fill = factor(N))) + 
      geom_bar(stat = "identity", position = "dodge") +
      labs(x = "Block Number", y = "Average Yield", fill = "Nitrogen") +
      ggtitle("Average Yield per Block, With and Without Nitrogen")
    
    avg_yield_plot
    ```

From the graph we can note that there seems to be a correlation between a higher *yield* and the presence of the additive *N* in the plot soil. We will further investigate this hypothesis with different tests such as a full two-way ANOVA. The *block* factor is not of interest but is applied to group plot tests and may introduce variability within the samples.

### Section c

A full two-way ANOVA is conducted with the response variable *yield* and the two factors *block* and *N*. We start with a main effects model.

```{r, echo=FALSE}
two_way_anova_main <- aov(yield ~ block + N, data = npk)
summary(two_way_anova_main)
```
We then compute an interaction model.
```{r, echo=FALSE}
two_way_anova_int <- aov(yield ~ block * N, data = npk)
summary(two_way_anova_int)
```

With a p-value of 0.0071 and 0.0262 respectively in the main effects model, the ANOVA signifies that both the *N* additive and *block* factor have statistically significant effects on the yield of peas with nitrogen having a larger effect compared to the block. However, we might say that the *block* factor merely introduces variability in the samples and do not represent fixed system effects. Including the *block* factor is, however, important to understand the amount of variability it may cause. In the interaction model both factors again have a p-value below 0.05. Additionally, we notice that there is no statistically significant effect on the *yield* from an interaction between *N* and *yield*.

To solidify our findings we perform an analysis on the residuals of the main effects model with a normality test and a plot of the distribution with a histogram.

```{r, echo=FALSE, fig.width = 6, fig.height = 4.5}
# Analysis of residuals (normality test)
qqnorm((residuals)(two_way_anova_main))
qqline(residuals(two_way_anova_main), col = 'red')

# Distribution in histogram (since slight deviation at tail)
hist(residuals(two_way_anova_main), main = "Histogram of Residuals", xlab = "Residuals", breaks = 10)

```

We note that there is a slight deviation at the tail of the normality test which we investigate this further with a Shapiro Wilk test.
```{r, echo=FALSE}
# Shapiro Wilk test (Since slight deviation at tail)
shapiro.test(residuals(two_way_anova_main))
```
From the Shapiro-Wilk test we receive a p-value greater than 0.05, combined with the Q-Q and histogram it is likely normally distributed.

Tests indicate a normal distribution of the residuals and the data is continuous. We can therefore say that a non-parametric designed test such as a Friedman test is not applicable. Additionally, We have limited in-block variation due to a randomized block design with continuous data which is more applicable to the ANOVA model design.

### Section d 

To investigate the effects of other variables and any other possible interactions we explore more models. We first test for the presence of additional main effects from all additives and the block factor.
```{r, echo=FALSE}
# Main effect model of all factors
model_1 <- aov(yield ~ N + P + K + block, data = npk)
summary(model_1)
```

We now learn that potassium has a statistically significant effect on the yield with a p-value of 0.02767. From this main effects model we therefore learn that only *P* does not contribute significantly, we will therefore omit *P* in subsequent models. 

Next, we explore the interaction between potassium and the other remaining main effect variables *N* and *block*. 

```{r, echo=FALSE}
# Interaction model between N, K and block
model_2 <- aov(yield ~ N * K + K * block, data = npk)
summary(model_2)
```
From the data we learn that there is no significant interaction between the different factors. We further explore this with an interaction plot.

```{r, echo=FALSE, fig.width = 6, fig.height = 4.5}
# Check to see if no interaction between variables
interaction.plot(npk$N, npk$block, npk$yield,
                 xlab = "Nitrogen (N)", 
                 ylab = "Yield", 
                 trace.label = "Block")
interaction.plot(npk$block, npk$N, npk$yield,
                 xlab = "Block", 
                 ylab = "Nitrogen (N)", 
                 trace.label = "Yield")
```

From this plot we only observe a minor possible interaction between *block* and *N*. However, due to the overall linearity we suspect the interaction to be mostly negligible. 

We will therefore approach a main effects model where we leave out phosphor to omit arbitrary data and prevent overfitting. The main effect model will include factors *N*, *K* and *block* 
```{r}
# NOTE: P could be omitted due to statistical insignificance
model_3 <- aov(yield ~ N + K + block, data = npk)
summary(model_3)
```

### Section e 
To investigate the influence of the factors on the *yield*, we will perform a Tukey's Honest Significant Difference (HSD) test.

```{r}
# Apply TukeyHSD 
tukey_results <- TukeyHSD(model_3)
tukey_results
```

From the TukeyHSD results we see that K has a significant negative impact on yield. Therefore we can say that the best combination for model 3 (where we do not consider *P*) is *N* = 1 and *K* = 0. Additionally, the *block* factor introduces variability with *block* 3 having the overall biggest positive impact and *block* 1 a relatively lower *yield*. 

### Section f 

Finally, we create a mixed effects analysis for our chosen main effect model with factors *N*, *K* and *block* where we model *block* as a random effect.
```{r, echo=FALSE}
# Create a Mixed effects model with all chosen factors.
mixed_effects_model <- lmer(yield ~ N + K + (1 | block), data = npk)
summary(mixed_effects_model)
```

Similarly to the additive model, we can conclude that *N* has a significant (5.617) positive effect on the *yield* with a p-value of 0.00302, whereas *K* has a statistically significant negative effect on the *yield* (-3.983) with a p-value of 0.02487. This analysis shows that *blocks* have significant variability in data, with a random effects variance of 13.16. This mixed effects model further solidifies the stance that *N* has a significant impact on *yield*. We further note that the mixed effects model may provide a better representation, since *blocks* introduce random variance and not fixed systemic effects such as *N*, *P* and *K*.