---
title: "Assignment 2 - Report"
author: "Eleni Liarou, Zoë Azra Blei, Frederieke Loth, group 20"
date: "`r Sys.Date()`"
output:
  pdf_document:
    keep_tex: true
    latex_engine: xelatex
fontsize: 11pt
highlight: tango
---

::: {style="text-align: justify;"}
```{r setup, echo=FALSE}
options(digits = 3)
```

# Exercise 1: Titanic

#### Section a

```{r, echo=FALSE}
data_titanic <- as.data.frame(read.table("titanic.txt", header=TRUE))
data_titanic$PClass <- as.factor(data_titanic$PClass)
data_titanic$Sex <- as.factor(data_titanic$Sex)
summary(data_titanic)
```

From the summary, we see that 3rd class passengers outnumbered those in 1st and 2nd classes combined. 35% of passengers were female, and 65% male. Half the passengers were aged between 21 and 39, with 557 missing age values. The survival rate has a mean of 0.3427, indicating around one-third of passengers survived. However, the dataset is incomplete, containing data for only 1,313 of the 2,224 passengers.

```{r, fig.height=3, fig.width=8, echo=FALSE}
#syrvival rate by sex, class and age
par(mfrow = c(1, 3))

# 1. Survival Rate by Sex
sex_survival <- table(data_titanic$Sex, data_titanic$Survived)
barplot(prop.table(sex_survival, margin = 1), beside = TRUE, 
        col = c("darkred", "darkgoldenrod1"), legend = c("Died", "Survived"),
        main = "Survival Rate by Sex", ylab = "Proportion", 
        names.arg = c("Female", "Male"))  # Explicitly set labels

# 2. Survival Rate by Passenger Class
# Create the contingency table
# Create the contingency table
class_survival <- table(data_titanic$Survived, data_titanic$PClass)

# Plot the bar plot
barplot(class_survival, beside = TRUE, 
        col = c("darkred", "darkgoldenrod1"), 
        legend.text = c("Died", "Survived"),  # Correct legend labels for survival
        main = "Survival Rate by Passenger Class", 
        ylab = "Proportion", 
        names.arg = c("1st", "2nd", "3rd"),  # Set labels for passenger class
        ylim = c(0, max(class_survival) * 1.1))  # Adjust y-axis to fit data

titanic_clean <- na.omit(data_titanic)

# 3. Survival Rate by Age (Histogram)
hist(titanic_clean$Age[titanic_clean$Survived == 1], breaks = 20, col = "darkgoldenrod1", 
     main = "Age Distribution of Survivors", xlab = "Age", freq = FALSE)

# Adding the "Died" histogram on top with a transparent red color
hist(titanic_clean$Age[titanic_clean$Survived == 0], breaks = 20, col = rgb(1, 0, 0, 0.5), 
     add = TRUE, freq = FALSE)

# Legend for the plot
legend("topright", legend = c("Survived", "Died"), fill = c("darkgoldenrod1", rgb(1, 0, 0, 0.5)))

```

More males than females died, and most 3rd class passengers did not survive. The highest number of both survivors and deaths were among passengers aged 20 to 30.

From the below graphs we see that the majority of passengers were around 25 years old, with fewer individuals in older age groups. 1st class passengers were generally older than those in 2nd and 3rd class.

```{r, fig.height=3, fig.width=7, echo=FALSE}
par(mfrow = c(1, 2))
 #5. Age Distribution by Class (Boxplot)
boxplot(Age ~ PClass, data = titanic_clean, main = "Age Distribution by Class", 
        col = c("lightblue", "lightgreen", "lightpink"), ylab = "Age")

hist(data_titanic$Age, xlab="Age", main="Age Distribution of Passengers", col="coral1")
```

Let's examine how the sexes were distributed over the passenger classes.

```{r}
sex_class <-xtabs(~PClass+Sex, data=data_titanic)
sex_class
```

As expected, since there are more males overall, each passenger class has a higher number of males.

```{r}
sex_class_surv <- xtabs(Survived~PClass+Sex, data=data_titanic)
round(sex_class_surv/sex_class, 2)
```

The survival rate decreases from 1st to 3rd class, but remains significantly higher for females across all classes. However, the difference in survival between females and males is less pronounced in 3rd class.

We will now fit a logistic regression model to investigate the association between the survival status and the predictors *PClass*, *Age* and *Sex.*

```{r}
add_mod <- glm(Survived ~ PClass + Age + Sex, data = titanic_clean, family = binomial)
summary(add_mod)$coefficients
cat("AIC:", AIC(add_mod))
```

The odds can be calculated using the estimates of the above table as:

$$
\text{odds} = e^{\text{log-odds}} = e^{3.7597 + (-1.292) \times \text{PClass2nd} + (-2.521) \times \text{PClass3rd} + (-0.0392) \times \text{Age} + (-2.631) \times \text{Sexmale}} 
$$

Being in 2nd or 3rd class significantly reduces the probability of survival compared to 1st class. Older age is associated with a lower likelihood of survival, while being male significantly decreases the chances of survival compared to being female. The magnitude of the coefficients reflects the sensitivity of survival odds to each variable. Being male has the largest negative effect on survival, while age has the smallest effect in comparison to class and sex.

#### Section b

Investigating the interaction between Age and PClass.

```{r}
summary(glm(Survived~Age*PClass, data=data_titanic, family="binomial"), test="Chisq")$coefficients
cat("AIC:", AIC(glm(Survived ~ Age * PClass, data=data_titanic, family="binomial")))
```

We now investigate the interaction between Age and Sex.

```{r}
summary(glm(Survived~Age*Sex, data=data_titanic, family="binomial"), test="Chisq")$coefficients
cat("AIC:", AIC(glm(Survived ~ Age * Sex, data=data_titanic, family="binomial")))
```

The interaction between Age and PClass does not have a significant effect on survival. The p-values for both Age:PClass2nd (0.405) and Age:PClass3rd (0.771) are greater than the 0.05 threshold, indicating that these interaction terms should not be included in the final model. The interaction between Age and Sex is statistically significant, with a p-value of 1.57e-06 \< 0.05 threshold. This suggests that the relationship between Age and survival differs by Sex, and thus, the interaction term should be included in the final model.

So the final model is:

```{r}
final_model <- glm(Survived ~ PClass + Age*Sex, data=data_titanic, family="binomial")
summary(final_model, test="Chisq")$coefficients
cat("AIC:", AIC(glm(Survived ~ PClass + Age*Sex, data=data_titanic, family="binomial")))
```

This model has an AIC of 679, which is lower than the additive model’s AIC of 705. Since a lower AIC indicates a better balance between fit and complexity, we choose this model over the additive one.

Using this model, we can now estimate the probability of survival for each combination of levels of the factors PClass and *Sex* for a person of age 55.

```{r, fig.height=4}
new_data <- expand.grid(Age = 55, PClass = levels(data_titanic$PClass), 
                        Sex = levels(data_titanic$Sex))
probabilities <- predict(final_model, newdata = new_data, type = "response")
cbind(new_data, Survival_Prob = round(probabilities, 3))
```

From the table above, we see once again that being female significantly increases the chance of survival, while survival probability decreases progressively from 1st to 3rd class.

#### Section c

To predict survival status, we split the data into training (80%) and testing (20%) subsets. We train a logistic regression model using glm() on the training set and use predict() to generate survival probabilities for the test set. After applying a threshold of 0.5 we classify passengers as survived or not. The quality of the prediction can be measured by accuracy (correct predictions/total cases) and other metrics such as AUC-ROC and precision or recall, which provide a more detailed evaluation, especially when dealing with class imbalance.

#### Section d

```{r, echo=FALSE}
cont_table_class <- table(data_titanic$Survived, data_titanic$PClass)
cont_table_sex <- table(data_titanic$Survived, data_titanic$Sex)
```

We will use Fisher's exact test to examine the effect of sex on survival status since it is more suitable for 2x2 tables and the 2-test to investigate the effect of class on survival status.

```{r, echo=FALSE}
fisher.test(cont_table_sex)
```

```{r, echo=FALSE}
chisq_result <- chisq.test(cont_table_class)

# Use cat to print the message and the values
cat("We performed a 2-test to investigate the relationship between PClass and Survived.\n")

cat("Chi-squared value:", chisq_result$statistic, "Degrees of freedom:", chisq_result$parameter, "p-value:", chisq_result$p.value, "\n")
```

Both tests show that class and sex are strongly associated with survival.

The 2-test for the relationship between passenger class and survival status reveals a significant association, with a test statistic of 172 and a p-value less than 2e-16. This indicates that survival is strongly influenced by passenger class, with the null hypothesis of no association being rejected. Similarly, Fisher’s Exact Test for gender and survival status shows a highly significant result (p-value \< 2e-16). The odds ratio of 0.1 suggests that females have much higher odds of surviving than males, with a 95% confidence interval (0.0762, 0.1316) confirming that the true odds ratio is significantly less than 1. This supports the conclusion that being female substantially increases the chances of survival.

#### Section e

The approach in (d) is not wrong; it simply tests for associations between categorical variables, whereas the approach in (a) and (b) allows for adjustment of multiple factors and prediction of survival probability.

**Logistic regression**

Advantages: Can handle both categorical and continuous predictors. Provides odds ratios, making interpretation straightforward. Allows for adjustment of multiple factors simultaneously. Can be used for predicting survival probability.

Disadvantages: Assumes a linear relationship between predictor and log-odds of the outcome. Can suffer from over-fitting if the sample size is too small.

**2-Test**

Advantages: Simple and easy to compute even for large datasets. Works well for categorical explanatory variables and can be applied to tables larger than 2x2.

Disadvantages: Less accurate for small sample sizes (expected counts \< 5 can make results unreliable). Only tells you whether dependence exists, not the nature of the dependency. Cannot be used for prediction.

**Fisher Test**

same as the 2-test except for the following

Advantages: Accurate for small sample sizes. Provides exact p-value

Disadvantages: Can be used for tables larger than 2x2 but is computationally expensive.

# Exercise 2: Military Coups

```{r echo=FALSE}
library(MASS)
data <- read.table("coups.txt", header = TRUE, row.names = 1)

# Transform 'pollib' into a factor, as we think it may not have a linear effect
data$pollib <- as.factor(data$pollib)
```

#### Section a
Note that we transformed *pollib* into a factor since we hypothesized the effect to not be strictly linear. This was found to be correct after comparing the different Poisson regression model outcomes. *pollib = 1* and *pollib = 2* are henceforth compared to the value of *pollib = 0*.

``` {r echo=FALSE}
poisson_reg <- glm (miltcoup ~ ., data = data, family = poisson)
summary(poisson_reg)
```

We determine that variables for which *p-value > 0.05* do not have a statistically significant effect on our response variable *miltcoup* which signifies the number of military coups. Moreover, a positive coefficient details that an increase in the corresponding variable indicates an increase in *miltcoup*. We can therefore state the following: *oligarchy* and *parties* are statistically significant with a positive effect on *miltcoup*. *pollib1* has a slightly significant and negative effect on *miltcoup* whilst *pollib2* has a significant negative effect. We can thus say that according to the data, countries with a higher political liberation factor may experience less military coups.

#### Section b
We will now remove variables from the model one-by-one to assess which variables are significant. For clarity we will only show the last model and comment on which variables were removed.

```{r echo=FALSE}
poisson_reg1 <- glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size + numregim, 
                    data = data,
                    family = poisson)

poisson_reg2 <- glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + numregim, 
                    data = data,
                    family = poisson)

poisson_reg3 <- glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn, 
                    data = data,
                    family = poisson)

poisson_reg4 <- glm(miltcoup ~ oligarchy + pollib + parties + pctvote, 
                    data = data,
                    family = poisson)
```

At each step, we removed the variable with the largest p-value for which *p-value > 0.05*. This resulted in the final poisson model with variables *oligarchy*, *pollib*, and *parties*. Although *pollib1* has a *p-value > 0.05*, we include it to properly reflect the stages within the variable.

``` {r}
# Everything p < 0.05
poisson_reg_final <- glm(miltcoup ~ oligarchy + pollib + parties, 
                    data = data,
                    family = poisson)
summary(poisson_reg_final)
```

This final version of the model is in compliance with our findings for the previously determined statistically significant variables.

#### Section c

We will now use the final model to make predictions about the mean number of coups per level of liberalization. We create a hypothetical country with mean values for the numerical variables *oligarchy*, and *parties*, while varying *pollib* for all levels (0, 1, and 2). 

``` {r echo = FALSE}
# Get all columns of which to take average
selected_vars <- c("oligarchy", "parties") 

# Compute the means
cols_means <- colMeans(data[, selected_vars])
cols_means

# Create dataset for prediction with pollib as a factor
data2 <- data.frame(pollib = factor(c(0, 1, 2), levels = levels(data$pollib)), 
                    t(cols_means))

# Make prediction
predicted_coups <- predict(poisson_reg_final, newdata = data2, type = 'response')
data.frame(pollib = c(0, 1, 2), predicted_coups = predicted_coups)

```
From this we find the mean for *oligarchy* to be *5.222* and the mean for *parties* to be *17.083*. Furthermore we find the number of predicted coups to decrease as the index for political liberalization increases. We find that our hypothetical country with no political liberalization experiences roughly *2.91* military coups, with limited rights it experiences an estimated *1.77* coups and under full political liberalization it is estimated to expect roughly *0.96* military coups. These results indicate that greater political liberalization is associated with fewer military coups.

# Exercise 3: Stormer viscometer

```{r, echo=FALSE}
library(MASS)
data(stormer)
library(ggplot2)
```

#### Section a

To understand the relationships between the variables, a scatterplot of the data is plotted below:

```{r,echo=FALSE, fig.width=6, fig.height=3}
ggplot(stormer, aes(x = Viscosity, y = Time, color = Wt)) +
  geom_point(size = 4) +  # Set point size
  scale_color_gradient(low = "purple", high = "yellow") + 
  labs(title = "Relationship Between Viscosity, Time, and Weight",
       x = "Viscosity (v)", y = "Time (T)", color = "Weight (w)") +
  theme_minimal() 
```

The scatterplot visualizes that higher weight values correspond to lower time values, demonstrating an inverse relationship between weight and time. Additionally, the pattern of points suggests a nonlinear relationship between viscosity and time, supporting the theoretical nonlinear model:

$$ 
T = \frac{\theta_1 v}{w - \theta_2} + e
$$ However, in order to estimate the parameters $\theta_1$ and $\theta_2$, since the theoretical model can be rewritten to a linear form, we can first apply linear regression to obtain initial estimates using the following formula: $$
wT = \theta_1 v + \theta_2 T + (w - \theta_2)e
$$ As the variance of the error term is not constant, we have to take into account heteroscedasticity. For this, the variance of $wT$ becomes the following: $$
Var(wT) = \sigma^2(w - \theta_2)^2
$$ We use Weighted Least Squares and set the weights in the regression as, using an initial guess for $\theta_2$ to do the linear regression: $$
w_i = \frac{1}{(w_i - \hat{\theta}_2)^2}
$$

```{r}
theta2_init = mean(stormer$Wt)
weights_wls = 1 / (stormer$Wt - theta2_init)^2
linear_model_wls = lm(Wt * Time ~ Viscosity + Time, data = stormer, weights = weights_wls)
summary(linear_model_wls)
```

The found estimated values are $\theta_1 = 26.499$ and $\theta_2 = 5.150$, where only the value of $\theta_1$ is statistically significant. Using these estimated values, we can do nonlinear regression:

```{r}
theta1_wls <- coef(linear_model_wls)["Viscosity"]
theta2_wls <- coef(linear_model_wls)["Time"]
nls_model_weighted <- nls(Time ~ (theta1 * Viscosity) / (Wt - theta2), 
                          data = stormer,
                          start = list(theta1 = theta1_wls, theta2 = theta2_wls),
                          weights = 1 / (Wt - theta2_wls)^2)

summary(nls_model_weighted)
```

The final estimated values are $\theta_1 = 29.637$ and $\theta_2 = 2.065$, where only $\theta_1$ seems statistically significant. Additionally, the residual standard error is smaller (0.342) compared to the linear model (22.96) indicating that this model has much less unexplained variation.

```{r, echo=FALSE, fig.width=6, fig.height=3}
viscosity_seq = seq(min(stormer$Viscosity), max(stormer$Viscosity), length.out = 100)
wt_values = unique(stormer$Wt)
plot_data = data.frame()

# Compute predicted Time for each viscosity and weight value using the weighted nonlinear model
for (w in wt_values) {
  pred_time <- predict(nls_model_weighted, 
                       newdata = data.frame(Viscosity = viscosity_seq, Wt = w))
  plot_data <- rbind(plot_data, data.frame(Viscosity = viscosity_seq, Time = pred_time, Weight = w))
}

# Scatterplot of original data with color by weight
ggplot(stormer, aes(x = Viscosity, y = Time, color = Wt)) +
  geom_point(size = 3) +
  scale_color_gradient(low = "purple", high = "yellow") +  
  geom_line(data = plot_data, aes(x = Viscosity, y = Time, group = Weight), 
            color = "red", size = 1.2) +
  labs(title = "Fitted Model for Time vs. Viscosity",
       x = "Viscosity (v)", y = "Time (T)",
       color = "Weight (w)") +
  theme_minimal() +
  ylim(0, 300)
```

The final plot shows that the nonlinear regression model fits the data well, capturing the expected trend between viscosity and time while accounting for different weight values. The fitted curves align closely with the data points, confirming that the nonlinear model is more appropriate than a linear model.However, the separation of curves suggests some variation in the effect of weight, and $\theta_2$ was not statistically significant, indicating potential refinements in the model.

#### Section b

A two-tailed t-test is conducted, as we have no expectation about whether $\theta_1$ will be greater or smaller than 25; it could differ in either direction. We consider the null hypothesis H0: $\theta_1$ = 25. For the test, we use the estimated $\theta_1$ and its standard error obtained from question a). The test resulted in a t-statistic of 4.81 and a p-value of 9.45e-05, which is far below the typical significance level of 0.05. This means we reject H0 and conclude that $\theta_1$ is significantly different from 25, further supporting the nonlinear model's results.

```{r}
theta1_hat = 29.4013  # Estimated parameter
theta1_se = 0.9155    # Standard error
theta1_h0 = 25        # Hypothesized value under H0
df = 21               # Degrees of freedom from nls summary

t_stat = (theta1_hat - theta1_h0) / theta1_se
p_value = 2 * pt(-abs(t_stat), df)
```

```{r,echo=FALSE,results='asis'}
rounded_p = formatC(p_value, format = "e", digits = 3)
cat("**Test Statistic (t):**", round(t_stat,3), "\n\n")
cat("**P-value:**", rounded_p, "\n")
```

#### Section c

For computing the 92% confidence interval for $\theta_1$ and $\theta_2$, we consider the following formula to calculate the z-value: $$
\hat{\theta} \pm z_{\alpha/2} \cdot SE(\theta)
$$ where $z_{\alpha/2}$ is the critical value from the standard normal distribution. For a 92% confidence level, the significance level is $\alpha$ = 0.08, thus: $$z_{0.04/2} = z_{0.02} \approx 1.75$$

This gave a 92% CI for $\theta_1$​ of [27.80, 31.00] and for $\theta_2$​ of [1.05, 3.38], meaning we are 92% confident that the true values lie within these intervals. Since the confidence interval for $\theta_1$​ does not include 25, it further supports rejecting H0​ from question b).

```{r}
theta1_hat <- 29.4013  # Estimated θ1
theta1_se <- 0.9155    # Standard error of θ1
theta2_hat <- 2.2183   # Estimated θ2
theta2_se <- 0.6655    # Standard error of θ2

z_value = qnorm(0.96)  # 1.75

theta1_CI = c(theta1_hat - z_value * theta1_se, theta1_hat + z_value * theta1_se)
theta2_CI = c(theta2_hat - z_value * theta2_se, theta2_hat + z_value * theta2_se)
```

```{r,echo=FALSE,results='asis'}
cat("**92% CI for θ1:** [", round(theta1_CI,3), "]\n\n")
cat("**92% CI for θ2:** [", round(theta2_CI,3), "]\n")
```

#### Section d

The expected values are computed using the nonlinear model with $w$ = 50, and viscosity values ranging from 10 to 300. The 94% confidence intervals were derived using asymptotic normality, where the standard error of T was estimated through error propagation. The confidence bounds were calculated as:

$$T(v) \pm z_{\alpha/2} \cdot SE(T)$$

where $z_{0.03}$ = 1.88 is the critical value for a 94% confidence level. The plot shows the expected T along with a shaded confidence band, indicating the uncertainty in our estimates. The confidence interval widens as viscosity increases, reflecting greater uncertainty for larger $v$. The linear trend suggests a strong relationship between viscosity and time, but further diagnostics are needed to confirm model assumptions. Overall, the plot aligns well with the theoretical nonlinear model, supporting its validity over a simple linear approximation.

```{}
```

```{r}
theta1_hat = 29.4013  # Estimate of theta1
theta2_hat = 2.2183   # Estimate of theta2
theta1_se = 0.9155    # Standard error of theta1
theta2_se = 0.6655    # Standard error of theta2
w_fixed = 50
v_values = seq(10, 300, length.out = 100)

T_hat = (theta1_hat * v_values) / (w_fixed - theta2_hat)
T_se = sqrt(
  (v_values / (w_fixed - theta2_hat))^2 * theta1_se^2 +
  (theta1_hat * v_values / (w_fixed - theta2_hat)^2)^2 * theta2_se^2
)

z_value = qnorm(0.97)
T_lower = T_hat - z_value * T_se
T_upper = T_hat + z_value * T_se
```

```{r,echo=FALSE, fig.width=6, fig.height=3}
df <- data.frame(v_values, T_hat, T_lower, T_upper)
ggplot(df, aes(x = v_values, y = T_hat)) +
  geom_line(color = "blue", linewidth = 1) +  
  geom_ribbon(aes(ymin = T_lower, ymax = T_upper), alpha = 0.2, fill = "blue") +
  labs(title = "94% Confidence Interval for Expected T",
       x = "Viscosity (v)", y = "Expected Time (T)") +
  theme_minimal()
```

#### Section e

To investigate whether the smaller model with $\theta_1$ = 25 is appropriate, we compare it to the estimated model using hypothesis testing and model evaluation metrics. From question b), the T-test rejected H0: $\theta_1$ =25 with a p-value of 9.45e-05, indicating that setting $\theta_1$ = 25 significantly deviates from the data. Additionally, the 92% confidence interval for $\theta_1$ of [27.80, 31.00] does not contain 25, further supporting that the restriction is inappropriate. A likelihood ratio test could be performed to formally compare the smaller model to the unrestricted model, however, the hypothesis test already suggests a poor fit. Constraining $\theta_1$ may lead to higher residual errors and reduced model flexibility, making the model less accurate. Given these findings, the smaller model does not seem appropriate, as it forces an assumption that contradicts the observed data. Therefore, the unrestricted nonlinear model remains the more valid choice.
:::
